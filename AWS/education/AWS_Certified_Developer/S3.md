<h1>S3</h1> 
<h4> 

[Back to menu](..%2F..%2FMenu.md)

</h4>

[//]: # (What about Aurora Security?)

<details>
    <summary>
        <b><big><big><big>
            What is Amazon S3?
        </big></big></big></b>
    </summary>

Amazon Simple Storage Service (Amazon S3) is an object storage service.
Has the following features:

- Storage classes (a range of storage classes for different storage scenarios)
- S3 Lifecycle (Customize the life cycle to suit your needs)
- S3 Object Lock (prevent objects from being deleted or overwritten for a 
fixed or indefinite time)
- S3 Replication - replication of objects and their corresponding metadata
- S3 Batch Operations - S3 Object Lambda - support for lambda operations
- Data processing - (tula for optimization)
- Event notifications
- A huge number of manual and automated analytics and storage monitoring tools
- Strong consistency (ensures reliable consistency of operations)

</details>
<br>

[//]: # (How Amazon S3 works?)

<details>
    <summary>
        <b><big><big><big>
            How Amazon S3 works?
        </big></big></big></b>
    </summary>

Amazon S3 is an object storage service that stores data as objects in buckets.
An object is a file and any metadata that describes the file.
A bucket is a container for items.

To store data in Amazon S3, you first create a bucket and specify 
a bucket name and AWS region.
The data is then uploaded to this bucket as objects in Amazon S3.
Each object has a key (or key name), which is the unique identifier 
of the object in the bucket.

</details>
<br>

[//]: # (What are buckets in S3?)

<details>
    <summary>
        <b><big><big><big>
            What are buckets in S3?
        </big></big></big></b>
    </summary>

A bucket is a container for objects stored in Amazon S3.
You can store any number of items in the Recycle Bin.
When you create a bucket, you enter a name for the bucket and 
select the AWS region in which it will be located.
The AWS account the user belongs to owns the bucket.
The name of the created cart is unique for all regions. 
The selected name is reserved by the account that created it.

</details>
<br>

[//]: # (Main Bucket naming rules?)

<details>
    <summary>
        <b><big><big><big>
            Main Bucket naming rules?
        </big></big></big></b>
    </summary>

- Segment names must contain from 3 to 63 characters.
- Segment names can only consist of lowercase letters, numbers, periods (.) and hyphens (-).
- Segment names must begin and end with a letter or number.
- Segment names must not be formatted as an IP address (for example, 192.168.5.4).
- Segment names must not begin with the prefix xn--.
- Segment names must not end with the suffix -s3alias.
- Segment names within a section must be unique.

Examples
- my-hosted-content
- my.example.s3.bucket
- doc-example-bucket- (ends with a hyphen)

</details>
<br>

[//]: # (S3 objects?)

<details>
    <summary>
        <b><big><big><big>
            S3 objects?
        </big></big></big></b>
    </summary>

Objects are core entities stored in Amazon S3.
Objects consist of object data and metadata.
Metadata is a set of name-value pairs that describe an object.
These pairs include some default metadata such as last modified date,
and standard HTTP metadata such as Content-Type.
You can also specify your own metadata when saving an object.

</details>
<br>

[//]: # (S3 Keys?)

<details>
    <summary>
        <b><big><big><big>
            S3 Keys?
        </big></big></big></b>
    </summary>

An object key (or key name) is a unique identifier for an object in the bucket.
Each object in the bucket has exactly one key.
A combination of segment, object key, and optionally version ID
(if S3 versioning is enabled for the bucket) uniquely identifies each object.
So you can think of Amazon S3 as
a basic data map between the “cart + key + version” and the object itself.

</details>
<br>

[//]: # (Replication?)

<details>
    <summary>
        <b><big><big><big>
            Replication?
        </big></big></big></b>
    </summary>

Replication provides automatic, asynchronous copying of objects to Amazon S3 buckets.

Replication can help with the following:
- Replicate objects while preserving metadata and encryption 
(This feature is important if you need to make sure
  that your replica is identical to the original object.)
- Replication of objects to different storage classes (in S3 Glacier,
S3 Glacier Deep Archive, etc.
  also move objects to a colder storage class (immediately necessary for lifecycle s3))
- Saving copies of objects with different owners (option for restricting access to replicas)
- Store objects in multiple AWS regions (allow for geographic differences
in where your data is stored)
- Replicate objects within 15 minutes (i.e. fast replication speed)

</details>
<br>

[//]: # (What steps are required to gain AWS CLI access to an S3 bucket?)

<details>
    <summary>
        <b><big><big><big>
            What steps are required to gain AWS CLI access to an S3 bucket?
        </big></big></big></b>
    </summary>

In order to be able to access s3 from the Amazon CLI,
it is necessary to give the IAM user access to s3 (via direct or group policies)
Next, configure a general or named profile in the CLI

</details>
<br>

[//]: # (How is it possible to control access to S3 resources?)

<details>
    <summary>
        <b><big><big><big>
            How is it possible to control access to S3 resources?
        </big></big></big></b>
    </summary>

- Writing custom AWS IAM policies
- Writing S3 policies (both general via UI and custom via JSON)
- Per-file access settings

</details>
<br>

[//]: # (Versioning)

<details>
    <summary>
        <b><big><big><big>
            S3 Versioning
        </big></big></big></b>
    </summary>

Versioning in Amazon S3 is a means of storing multiple versions 
of an object in a single bucket.
You can use the S3 versioning feature to save,
retrieving and restoring every version of every object stored in your recycle bins.

</details>
<br>

[//]: # (How is it possible to optimize cost of S3 resources?)

<details>
    <summary>
        <b><big><big><big>
            How is it possible to optimize cost of S3 resources?
        </big></big></big></b>
    </summary>

1. Organization at the bucket level
   (Understanding how and when your data is collected,
   access and archiving or deletion by users is critical to managing storage costs.)

2. Organization at the object level
   (By organizing access to objects you can save money)

3. choosing the right Amazon S3 storage class 
(they differ in the type of life cycle files and how to work with them)

</details>
<br>

[//]: # (How nested file hierarchies are represented in S3?)

<details>
    <summary>
        <b><big><big><big>
            How nested file hierarchies are represented in S3?
        </big></big></big></b>
    </summary>

**The prefix** is the full path before the object name, which includes the segment name.
BucketName/Project/WordFiles/123.txt, the prefix is “BucketName/Project/WordFiles/”

**Folder** is the value between the two "/" characters. For example, if the file is stored as
BucketName/Project/WordFiles/123.txt, the file path indicates that there is a folder
(“Project”) and subfolders (“WordFiles”). 
Both "Project" and "WordFiles" are considered folders.

</details>
<br>

[//]: # (What is CloudFront?)

<details>
    <summary>
        <b><big><big><big>
            What is CloudFront? 
            What is Edge Location/Edge Origin/Distribution?
        </big></big></big></b>
    </summary>

System of distributed servers which deliver webpages and web content

- **Edge Location** - the location where content is cached. Separate on region/AZ
- **Edge Origin** - The origin area from hum files will be distributed.
    (it can be s3, ec2, elastic load balancer, route53)
- **CF Distribution** - the name of origin and content to distribute 

</details>
<br>

[//]: # (What is CORS?)

<details>
    <summary>
        <b><big><big><big>
            What is CORS?
        </big></big></big></b>
    </summary>

Cross-Origin Resource sharing is used to allow resources in one
S3 bucket to access resources located in another S3 bucket

</details>
<br>

[//]: # (S3 Encrytion?)

<details>
    <summary>
        <b><big><big><big>
            S3 Encrytion?
        </big></big></big></b>
    </summary>

**In Transit**
- SSL/TLS
- HTTPS

**In Rest side**
- Server-side encryption
- SSE-S3 (AES 256-bit)
- SSE-KMS
- SSE-C

**Client-Side Encryption**
- you need to encrypt the files yourself before you upload them

**SSE creation encryption**
- Encryption using SSE-S3 is enabled by default while s3 is created

</details>
<br>

[//]: # (What steps need to do to setup your website on S3?)

<details>
    <summary>
        <b><big><big><big>
            What steps need to do to setup your website on S3?
        </big></big></big></b>
    </summary>

- Edit the bucket's Block Public Access settings and add a public bucket policy
- Create an index and error document
  (An index document (usually index.html) serves as the homepage of your website.
Specifying this ensures visitors are directed to the correct starting page. 
Similarly, setting an error document provides a user-friendly 
way to handle requests for URLs that do not exist on your website.)
- Enable static website hosting
- Create an S3 bucket in the region nearest to most of your users

</details>
<br>

[//]: # (What is S3 access points?)

<details>
    <summary>
        <b><big><big><big>
            What is S3 access points?
        </big></big></big></b>
    </summary>

Amazon S3 access points simplify data access for any AWS service 
or customer application that stores data in S3. 

Access points are named network endpoints that are attached to buckets 
that you can use to perform S3 object operations, 
such as GetObject and PutObject

Usage example, you can have S3 object with personal data 
and Lambda function with deletion of this data. 
You can create an access point from the S3 console 
to start that function on GetObject operation.

And you will have two types of one file. 
- One is via object with private data.
- One via access point without private data

</details>
<br>

[//]: # (What is S3 access points?)

<details>
    <summary>
        <b><big><big><big>
            Is it possible to configure S3 to insert object data to DB after put operation?
        </big></big></big></b>
    </summary>

Configure an S3 event to invoke an AWS Lambda function 
that inserts records into DynamoDB.

</details>
<br>